[
  {
    "objectID": "37.dot_density.html",
    "href": "37.dot_density.html",
    "title": "37. Dot Density Maps",
    "section": "",
    "text": "Via cornell-gis on Youtube.\nThis tutorial shows you how to make a racial dot density map like the ones we viewed in class\nDot density is an alternative to choropleths to show concentration, especially when each dot represents a number of people, households, occurrences, etc.\nIt is more common to display a single variable with dot density, as opposed to multiple as the racial dot maps do.\nMore info: Dot Density Maps",
    "crumbs": [
      "QGIS",
      "37. Dot Density Maps"
    ]
  },
  {
    "objectID": "37.dot_density.html#video-tutorial",
    "href": "37.dot_density.html#video-tutorial",
    "title": "37. Dot Density Maps",
    "section": "",
    "text": "Via cornell-gis on Youtube.\nThis tutorial shows you how to make a racial dot density map like the ones we viewed in class\nDot density is an alternative to choropleths to show concentration, especially when each dot represents a number of people, households, occurrences, etc.\nIt is more common to display a single variable with dot density, as opposed to multiple as the racial dot maps do.\nMore info: Dot Density Maps",
    "crumbs": [
      "QGIS",
      "37. Dot Density Maps"
    ]
  },
  {
    "objectID": "36.heatmaps.html",
    "href": "36.heatmaps.html",
    "title": "36. Heatmaps",
    "section": "",
    "text": "Via GeoDelta Labs on Youtube.\nHeatmaps show the concentration of many points in close proximity.\nYou can use this tutorial with your own point dataset or NYC Crashes as an example. NYC Crashes has 2M+ rows, so I recommend filtering it in the Socrata portal or in R before importing to QGIS. This filtered view has all crashes in October 2023.\nAfter adding your own basemap and bringing in the CSV as a spatial dataset, you can start at 9:20 in the video.",
    "crumbs": [
      "QGIS",
      "36. Heatmaps"
    ]
  },
  {
    "objectID": "36.heatmaps.html#video-tutorial",
    "href": "36.heatmaps.html#video-tutorial",
    "title": "36. Heatmaps",
    "section": "",
    "text": "Via GeoDelta Labs on Youtube.\nHeatmaps show the concentration of many points in close proximity.\nYou can use this tutorial with your own point dataset or NYC Crashes as an example. NYC Crashes has 2M+ rows, so I recommend filtering it in the Socrata portal or in R before importing to QGIS. This filtered view has all crashes in October 2023.\nAfter adding your own basemap and bringing in the CSV as a spatial dataset, you can start at 9:20 in the video.",
    "crumbs": [
      "QGIS",
      "36. Heatmaps"
    ]
  },
  {
    "objectID": "26.basemaps_points.html",
    "href": "26.basemaps_points.html",
    "title": "26. Basemaps and point symbology",
    "section": "",
    "text": "Add a basemap from an external web server to show the full area surrounding your data, with various levels of detail. Symbolize points at different sizes and with transparency to have them show up clearly at different map scales.",
    "crumbs": [
      "QGIS",
      "26. Basemaps and point symbology"
    ]
  },
  {
    "objectID": "26.basemaps_points.html#video-tutorial",
    "href": "26.basemaps_points.html#video-tutorial",
    "title": "26. Basemaps and point symbology",
    "section": "",
    "text": "Add a basemap from an external web server to show the full area surrounding your data, with various levels of detail. Symbolize points at different sizes and with transparency to have them show up clearly at different map scales.",
    "crumbs": [
      "QGIS",
      "26. Basemaps and point symbology"
    ]
  },
  {
    "objectID": "26.basemaps_points.html#data-downloads",
    "href": "26.basemaps_points.html#data-downloads",
    "title": "26. Basemaps and point symbology",
    "section": "Data downloads",
    "text": "Data downloads\nCSV of geocoded free lunch locations during COVID\nNYC borough boundaries",
    "crumbs": [
      "QGIS",
      "26. Basemaps and point symbology"
    ]
  },
  {
    "objectID": "20.intro_crs.html",
    "href": "20.intro_crs.html",
    "title": "20. Introduction to Coordinate Reference Systems",
    "section": "",
    "text": "Understand the basics of Coordinate Reference Systems (CRSs) and map projections. See how they function based on the layers you add and learn to adjust your project CRS.\nLearn more about troubleshooting CRS issues: I Hate Coordinate Systems!",
    "crumbs": [
      "QGIS",
      "20. Introduction to Coordinate Reference Systems"
    ]
  },
  {
    "objectID": "20.intro_crs.html#video-tutorial",
    "href": "20.intro_crs.html#video-tutorial",
    "title": "20. Introduction to Coordinate Reference Systems",
    "section": "",
    "text": "Understand the basics of Coordinate Reference Systems (CRSs) and map projections. See how they function based on the layers you add and learn to adjust your project CRS.\nLearn more about troubleshooting CRS issues: I Hate Coordinate Systems!",
    "crumbs": [
      "QGIS",
      "20. Introduction to Coordinate Reference Systems"
    ]
  },
  {
    "objectID": "20.intro_crs.html#data-downloads",
    "href": "20.intro_crs.html#data-downloads",
    "title": "20. Introduction to Coordinate Reference Systems",
    "section": "Data downloads",
    "text": "Data downloads\nBorough boundaries\nNYC subway stations (Saved copy)\nStormwater outfalls:\n\nvia NYS GIS Clearinghouse - download GeoJSON format for clearest difference in CRS\nSaved copy",
    "crumbs": [
      "QGIS",
      "20. Introduction to Coordinate Reference Systems"
    ]
  },
  {
    "objectID": "15.apis.html#downloading-data-from-the-web",
    "href": "15.apis.html#downloading-data-from-the-web",
    "title": "15. APIs",
    "section": "Downloading Data from the web",
    "text": "Downloading Data from the web\nYou can download datasets directly from the web with read_csv().\n\nread_csv('https://raw.githubusercontent.com/Statology/Miscellaneous/main/basketball_data.csv')\n\n# A tibble: 5 × 3\n  player assists points\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 A            6     12\n2 B            7     19\n3 C           14      7\n4 D            4      6\n5 E            5     10\n\n\nBut sometimes getting data stored on the web is more complicated, or you are accessing massive databases that would take forever to read the whole thing into R.\nWhen downloading data from NYC Open Data and other sources, you can use a special set of commands to filter and manipulate the data before downloading it to your local computer. These commands, unlike ones you’d write in an R script, are actually written in the URL itself. This kind of command is called an “API Call”— API stands for “Application programming interface” and is just a special system and language for users to communicate with the back end data servers. API Calls to APIs use a special syntax, which will be included in an API’s documentation. For NYC’s Open data, that syntax is very similar to a popular language called SQL or Structured Query Language.\nSome APIs will require registration of a “token” which will be an input in your URL. We’ll go over those with an example using the census API.",
    "crumbs": [
      "Learning R",
      "15. APIs"
    ]
  },
  {
    "objectID": "15.apis.html#writing-api-calls-to-nyc-open-data",
    "href": "15.apis.html#writing-api-calls-to-nyc-open-data",
    "title": "15. APIs",
    "section": "Writing API Calls to NYC Open Data",
    "text": "Writing API Calls to NYC Open Data\nUsing API Calls is particularly useful when you’re dealing with a **huge dataset** that would otherwise be a hassle to download in full. For this example, we’re going to use the HPD’s Housing Maintinance Code Violations dataset on NYC Open Data, which has ~7 million rows.\n\nGrabbing the “API Endpoint”\nLet’s first grab the beginning part of the URL that we’re going to use to write our API Call. This is called the “API Endpoint” and can be found by clicking the “API” tab on the NYC Open Data page for the dataset you’re working with.\nBefore copying the URL, make sure you set the data format toggle from “JSON” to “CSV”, as that is the format we’re going to want our data in.\nFor our example, our endpoint looks like this:\nhttps://data.cityofnewyork.us/resource/wvxf-dwi5.csv\n\n\nWriting up your API Call\nCopy the API Endpoint into a text editor - or an R script! - (I prefer Sublime Text or Visual Studio Code— others like Word or Pages have a tendency to “auto-correct” certain letters and syntax which may mess you up).\nNow, to initiate our query, we are going to add ?$query= to the end of our URL:\nhttps://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$query=\n\n\nAdding your Query\nAt the end our URL, we can now add some special code to filter the violations data for our download.\nTo do this, we’re going to want to first take a look at the API Documentation for our dataset of choice, which can be found by clicking on the “API” tab again on the dataset’s Open Data page and clicking the “API Docs” button. Specifically, this documentation gives us a run down of all of the columns in the data and how we can reference them by name in our API call.\nFor this example, we want to look at the most serious (class C) HPD Violations within the past month. So, we’re going to write out our query as such:\nSELECT * – this selects all columns of the data\nWHERE inspectiondate&gt;='2021-06-01T00:00:01.000' AND inspectiondate&lt;'2021-09-01T00:00:01.000' AND class='C' – this filters only rows where the inspectiondate value is between June 1st and Aug 31st, and the class of the violation is 'C'. The AND operator here allows us to include multiple filtering conditions at once, and could even include conditions on other columns. Note the special format that the dates come in… we were able to spot this by looking at the Documentation.\nLIMIT 100000 — this sets the maximum number of downloadable rows to 100,000. It’s good practice to set a limit here so we don’t accidentially try downloading millions of rows at once. Note: if you don’t specify, the default limit is just 1,000 rows!\nYou can find more information on the types of queries you can write on the Socrata Developers Portal (Socrata is the special “flavor” of API that NYC Open Data uses).\n\n\nRunning our API Call\nWe add the above pieces in that order to our URL:\nhttps://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$query=SELECT * WHERE inspectiondate&gt;='2021-06-01T00:00:01.000' AND inspectiondate&lt;'2021-09-01T00:00:01.000' AND class='C' LIMIT 100000\nNow, you can copy this full url into your browser and press ENTER— your special download should begin!\nWe can also use string operators to create and quickly modify the different components of our API call.\n\nbase_url &lt;- \"https://data.cityofnewyork.us/resource/wvxf-dwi5.csv\"\n\ninspectiondate_range &lt;- c(\"2021-06-01T00:00:01.000\",\"2021-09-01T00:00:01.000\")\n\nclass &lt;- \"C\"\n\nlimit &lt;- c(\"100000\")\n\n\nfull_api_call &lt;- paste0(base_url, \"?$query=SELECT * WHERE inspectiondate&gt;='\",inspectiondate_range[1],\"' AND inspectiondate&lt;'\",inspectiondate_range[2],\"' AND class='\",class,\"' LIMIT \",limit)\n\n\n\nImporting your data directly into R Studio\nOnce you have your data downloaded via API Call, you can feel free to import it into your R project like any other CSV. If you want to use the URL you created to import it directly, you can do that as well:\n\nlibrary(tidyverse)\nlibrary(fs)\n\n# R doesn't like weird characters like spaces and carats, so we need the `URLencode` function here to encode those symbols properly\n\nurl_hpd_viol &lt;- URLencode(\"https://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$query=SELECT * WHERE inspectiondate&gt;='2021-06-01T00:00:01.000' AND inspectiondate&lt;'2021-09-01T00:00:01.000' AND class='C' LIMIT 100000\")\n\n\n# Now, we can use our formatted url inside our `read_csv` function\n\nsummer_hpd_viols &lt;- read_csv(url_hpd_viol)\n\n\n\nNote: Always check the size of your output\nSometimes, the limit on your API Call may make your data export smaller than your desired outcome, and you won’t necessarily be notified. Therefore, it is always very important to check the number of rows of your data from your API Call before proceeding with analysis— if the number of rows matches the exact number of your limit (or is 1000, the default limit), it’s very likely that your data got cut off and you don’t have the complete set of data that you wanted.\nThe below example illustrates this problem and shows how to diagnose. For the example, imagine that we didn’t include a LIMIT clause in our API Call query:\nhttps://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$query=SELECT * WHERE inspectiondate&gt;='2021-06-01T00:00:01.000' AND inspectiondate&lt;'2021-09-01T00:00:01.000'\n\nurl_viol_no_limit = URLencode(\"https://data.cityofnewyork.us/resource/wvxf-dwi5.csv?$query=SELECT * WHERE inspectiondate&gt;='2021-06-01T00:00:01.000' AND inspectiondate&lt;'2021-09-01T00:00:01.000'\")\n\nsummer_violations_cut_off &lt;- read_csv(url_viol_no_limit)\n\n# Using the head() function won't actually reveal the cut-off problem:\nhead(summer_violations_cut_off)\n\n# A tibble: 6 × 41\n  violationid buildingid registrationid boroid boro   housenumber lowhousenumber\n        &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;         \n1    14369016     326090         374306      3 BROOK… 735         735           \n2    14368090     374043         316185      3 BROOK… 565         565           \n3    14372713     123595         219960      2 BRONX  4639        4639          \n4    14372260     167073         320072      3 BROOK… 416         416           \n5    14372287     167073         320072      3 BROOK… 416         416           \n6    14338217      38732         122934      1 MANHA… 414         414           \n# ℹ 34 more variables: highhousenumber &lt;chr&gt;, streetname &lt;chr&gt;,\n#   streetcode &lt;dbl&gt;, zip &lt;dbl&gt;, apartment &lt;chr&gt;, …\n\n\nBy looking at the head of your dataset, things appear to be fine. However, let’s use the nrow() function to get a sense of how many rows we have:\n\nnrow(summer_violations_cut_off)\n\n[1] 1000\n\n\nGiven that our data output is 1,000 rows, which is exactly the default limit for API Calls to NYC Open Data, it’s very likely that our data got cut off and there are more rows within our filtering conditions that we want.\nOur next step would be to increase our LIMIT in our API CAll until get a number of outputs rows below the limit value. In our first example, you can see we’ve done just that— our LIMIT was set to 100,000 rows and we only received around 20K or so rows. Safe to say we got all of the rows that fit our filtering criteria…",
    "crumbs": [
      "Learning R",
      "15. APIs"
    ]
  },
  {
    "objectID": "15.apis.html#other-apis",
    "href": "15.apis.html#other-apis",
    "title": "15. APIs",
    "section": "Other APIs",
    "text": "Other APIs\nThousands of sites have APIs that work similarly. Each API will have specific documentation and formatting. Once you read up on the documentation for one, the others will be easier to decipher. Here are some examples\n\nSpotify\nPro Publica’s Nonprofit Explorer\nBureau of Labor Statistics (BLS)",
    "crumbs": [
      "Learning R",
      "15. APIs"
    ]
  },
  {
    "objectID": "15.apis.html#api-wrappers",
    "href": "15.apis.html#api-wrappers",
    "title": "15. APIs",
    "section": "API Wrappers",
    "text": "API Wrappers\nA number of developers have also created R packages that make certain APIs easier to use. The example we will look at next, tidycensus, is an example of a wrapper for the US Census API. It will construct API calls based on parameters we set in a function. Here are some other API wrappers.\n\nRSocrata (Many Open Data Portals, including NYC’s use Socrata’s data system)\nBLS\nGoogle Sheets\nDatawrapper",
    "crumbs": [
      "Learning R",
      "15. APIs"
    ]
  },
  {
    "objectID": "22.spatial_csv.html",
    "href": "22.spatial_csv.html",
    "title": "22. Adding a spatial CSV layer",
    "section": "",
    "text": "How to add a CSV that contains latitude and longitude to your map project as a point layer",
    "crumbs": [
      "QGIS",
      "22. Adding a spatial CSV layer"
    ]
  },
  {
    "objectID": "22.spatial_csv.html#video-tutorial",
    "href": "22.spatial_csv.html#video-tutorial",
    "title": "22. Adding a spatial CSV layer",
    "section": "",
    "text": "How to add a CSV that contains latitude and longitude to your map project as a point layer",
    "crumbs": [
      "QGIS",
      "22. Adding a spatial CSV layer"
    ]
  },
  {
    "objectID": "22.spatial_csv.html#data-downloads",
    "href": "22.spatial_csv.html#data-downloads",
    "title": "22. Adding a spatial CSV layer",
    "section": "Data downloads",
    "text": "Data downloads\nCSV of geocoded free lunch locations during COVID\nBorough boundaries",
    "crumbs": [
      "QGIS",
      "22. Adding a spatial CSV layer"
    ]
  },
  {
    "objectID": "19.adding_spatial_layers.html",
    "href": "19.adding_spatial_layers.html",
    "title": "19. Adding and exploring spatial layers",
    "section": "",
    "text": "Add New York City boroughs and subway station shapefiles as layers to your QGIS project and explore the data that they contain using the Attribute Table and the Identify Features tool.",
    "crumbs": [
      "QGIS",
      "19. Adding and exploring spatial layers"
    ]
  },
  {
    "objectID": "19.adding_spatial_layers.html#video-tutorial",
    "href": "19.adding_spatial_layers.html#video-tutorial",
    "title": "19. Adding and exploring spatial layers",
    "section": "",
    "text": "Add New York City boroughs and subway station shapefiles as layers to your QGIS project and explore the data that they contain using the Attribute Table and the Identify Features tool.",
    "crumbs": [
      "QGIS",
      "19. Adding and exploring spatial layers"
    ]
  },
  {
    "objectID": "19.adding_spatial_layers.html#data-downloads",
    "href": "19.adding_spatial_layers.html#data-downloads",
    "title": "19. Adding and exploring spatial layers",
    "section": "Data downloads",
    "text": "Data downloads\nBorough boundaries\nNYC subway stations (Saved copy)",
    "crumbs": [
      "QGIS",
      "19. Adding and exploring spatial layers"
    ]
  },
  {
    "objectID": "21.geocoding.html",
    "href": "21.geocoding.html",
    "title": "21. Geocoding addresses",
    "section": "",
    "text": "How to geocode, i.e. add latitude and longitude to a CSV/tabular dataset that only contains addresses, which will allow you to plot the CSV on a map.\nThis tutorial uses Geocodio",
    "crumbs": [
      "QGIS",
      "21. Geocoding addresses"
    ]
  },
  {
    "objectID": "21.geocoding.html#video-tutorial",
    "href": "21.geocoding.html#video-tutorial",
    "title": "21. Geocoding addresses",
    "section": "",
    "text": "How to geocode, i.e. add latitude and longitude to a CSV/tabular dataset that only contains addresses, which will allow you to plot the CSV on a map.\nThis tutorial uses Geocodio",
    "crumbs": [
      "QGIS",
      "21. Geocoding addresses"
    ]
  },
  {
    "objectID": "21.geocoding.html#data-downloads",
    "href": "21.geocoding.html#data-downloads",
    "title": "21. Geocoding addresses",
    "section": "Data downloads",
    "text": "Data downloads\nCOVID meals dataset without latitude and longitude to practice\nOriginal COVID meals dataset",
    "crumbs": [
      "QGIS",
      "21. Geocoding addresses"
    ]
  },
  {
    "objectID": "2.r_projects.html#opening-r-studio",
    "href": "2.r_projects.html#opening-r-studio",
    "title": "2. Creating R Projects",
    "section": "Opening R Studio",
    "text": "Opening R Studio\nWhen you load RStudio, the screen will be split in three. This is some important vocabulary to direct yourself around RStudio and get help when you need it.\n\nThe CONSOLE is where all your code is run. Here you will see the output from the code you run. The COMMAND LINE is where you can type in code and execute it to the console. Code you run in the console is not saved. Try typing 1+1 into the console, and you’ll see that R spits back 2.\nThe ENVIRONMENT is where all your data will be stored. R is an object oriented programming language. Think of it like having a bunch of spreadsheets open at once. The environment shows you all the data you have loaded, and what each dataset, list, or other object is called.\nThe DIRECTORY is in the bottom right. This links to all the files in your current folder, called your working directory. If you are ever in the wrong working directory, you can set it by running the setwd() function or going to “Session” -\\&gt; “Set Working Directory.” We will keep files organized by using an R Project.",
    "crumbs": [
      "Learning R",
      "2. Creating R Projects"
    ]
  },
  {
    "objectID": "2.r_projects.html#creating-a-project",
    "href": "2.r_projects.html#creating-a-project",
    "title": "2. Creating R Projects",
    "section": "Creating a Project",
    "text": "Creating a Project\nAn R Project is basically a folder that will hold all your files together in one place - including your code, raw data, and any output you may produce.\nCreate your first R Project by clicking on the projects icon in the top right. You can create a project from a new or existing directory.\nWhen you return to RStudio to work on a saved project, open the project again by using the Project menu in RStudio, or double clicking the .Rproj file in the project directory.",
    "crumbs": [
      "Learning R",
      "2. Creating R Projects"
    ]
  },
  {
    "objectID": "2.r_projects.html#creating-your-first-script",
    "href": "2.r_projects.html#creating-your-first-script",
    "title": "2. Creating R Projects",
    "section": "Creating Your First Script",
    "text": "Creating Your First Script\nGo to File -&gt; New Script and save it to your project folder\nUse the assignment operator &lt;- to save values, dataframes, and other objects to the environment for future use.\nUse command+enter (Mac) or ctrl + enter (Windows) to run your code. Or select all and then run the shortcut to run the whole script at once.",
    "crumbs": [
      "Learning R",
      "2. Creating R Projects"
    ]
  },
  {
    "objectID": "1.installing_r.html",
    "href": "1.installing_r.html",
    "title": "1. Installing R",
    "section": "",
    "text": "To get started with this tutorial visit the RStudio download page",
    "crumbs": [
      "Learning R",
      "1. Installing R"
    ]
  },
  {
    "objectID": "1.installing_r.html#video-tutorial",
    "href": "1.installing_r.html#video-tutorial",
    "title": "1. Installing R",
    "section": "",
    "text": "To get started with this tutorial visit the RStudio download page",
    "crumbs": [
      "Learning R",
      "1. Installing R"
    ]
  },
  {
    "objectID": "1.installing_r.html#installing-r-and-rstudio",
    "href": "1.installing_r.html#installing-r-and-rstudio",
    "title": "1. Installing R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\nTo use R, we need R, the underlying language that runs our code, as well as R studio, an integrated development environment (IDE) that allows us to save files and edit and run code.\nTo get started, we’ll visit RStudio’s homepage to install R.\nFirst, Install R for your operating system from CRAN.\nThen download the latest version of RStudio and open it.",
    "crumbs": [
      "Learning R",
      "1. Installing R"
    ]
  },
  {
    "objectID": "12.ggplot.html#preparing-data-for-ggplot",
    "href": "12.ggplot.html#preparing-data-for-ggplot",
    "title": "12. ggplot",
    "section": "Preparing Data for ggplot",
    "text": "Preparing Data for ggplot\nData for ggplot needs to be in “long” format, meaning that all the values need to be in the same column and all the variables need to be in another column.\nLet’s use that rent stabilization data to create a plot. In this case the data is wide by year, meaning there is a column for each year with different values. We want a dataset where each year has its own row - where each row is a year-building combination - that we can plot.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nrent_stab_long &lt;- read_csv(\"https://taxbillsnyc.s3.amazonaws.com/joined.csv\") %&gt;% \n  select(borough, ucbbl, ends_with(\"uc\")) %&gt;% \n  pivot_longer(\n    ends_with(\"uc\"),  # The multiple column names we want to mush into one column\n    names_to = \"year\", # The title for the new column of names we're generating\n    values_to = \"units\" # The title for the new column of values we're generating\n  )",
    "crumbs": [
      "Learning R",
      "12. ggplot"
    ]
  },
  {
    "objectID": "12.ggplot.html#ggplot-syntax",
    "href": "12.ggplot.html#ggplot-syntax",
    "title": "12. ggplot",
    "section": "ggplot syntax",
    "text": "ggplot syntax\nggplots use similar syntax to regular R operations - they are groups of functions beginning with ggplot(). Instead of the pipe, ggplot uses + between different functions to build layers on top of each other.\nTypically, ggplot will start with ggplot(dataframe) + a geometric function like geom_col() and an aesthetics or aes argument that indicates which variables to plot.\n\n\nAfter that, ggplot has a ton of options to specify the labels, scales, axes, themes, legends,and more. It’s best shown through examples",
    "crumbs": [
      "Learning R",
      "12. ggplot"
    ]
  },
  {
    "objectID": "12.ggplot.html#sample-line-chart",
    "href": "12.ggplot.html#sample-line-chart",
    "title": "12. ggplot",
    "section": "Sample Line Chart",
    "text": "Sample Line Chart\nLine charts are often a good fit for time-series data. Let’s summarize the long data by year and borough and count the number of units.\n\nrs_long_manhattan_summary &lt;- rent_stab_long %&gt;% \n  filter(borough %in% c(\"MN\",\"BK\") # Filter only Manhattan and Brooklyn values\n          & !is.na(units)) %&gt;% # Filter out null unit count values\n  mutate(year = as.numeric(gsub(\"uc\",\"\", year))) %&gt;% # Remove \"uc\" from year values\n  select(year, borough, units) %&gt;% \n  # Grouping by 2 columns means each row will have a unique pair of the two columns' values.\n  # Our rows will look like: 2007 MN, 2007 BK, 2008 MN... \n  group_by(year, borough) %&gt;% \n  summarise(total_units = sum(units))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nNow let’s plot it. We start with the data, then add aesthetics, our chart type (line), axis specifications, and labels.\n\nrs_over_time_graph &lt;- ggplot(rs_long_manhattan_summary) +\n    # Note these arguments inside 'geom_line' :\n  geom_line(aes(x=year, y=total_units, color=borough)) +\n    # Restyle the Y-axis labels: \n  scale_y_continuous(\n    limits = c(0,300000),\n    labels = scales::unit_format(scale = 1/1000, unit=\"K\")) +\n  scale_x_continuous(breaks = seq(2007, 2017, by = 1))+\n    # Restyle the Legend: \n  scale_fill_discrete(\n    name=\"Borough\",\n    breaks=c(\"BK\", \"MN\"),\n    labels=c(\"Brooklyn\", \"Manhattan\")) +\n  labs(\n    title = \"Total Rent Stabilized Units over Time\",\n    subtitle = \"Manhattan and Brooklyn, 2007 to 2017\",\n    x = \"Year\",\n    y = \"Total Rent Stabilized Units\",\n    caption = \"Source: taxbills.nyc\"\n  )+\n  theme_minimal()\n\nrs_over_time_graph",
    "crumbs": [
      "Learning R",
      "12. ggplot"
    ]
  },
  {
    "objectID": "12.ggplot.html#sample-bar-chart-faceting",
    "href": "12.ggplot.html#sample-bar-chart-faceting",
    "title": "12. ggplot",
    "section": "Sample Bar Chart + Faceting",
    "text": "Sample Bar Chart + Faceting\nLet’s make some bar charts out of the table of # of children by the languages they speak at home by borough.\nWe’ll learn how we pulled this data in an upcoming lesson, but for now just download this csv. Or run read_csv(\"https://raw.githubusercontent.com/pspauster/learning_R/master/langs_by_boro.csv\")\n\nlangs_by_boro_for_graphing &lt;- read_csv(\"langs_by_boro.csv\") %&gt;%\n  mutate(\n    labels = # Add a new column with neat category labels\n      case_when(\n        variable == 'englishkids' ~ 'English',\n        variable == 'spanishkids' ~ 'Spanish',\n        variable == 'indoeurkids' ~ 'Indo-European',\n        variable == 'apikids' ~ 'Asian & Pacific Islander',\n        variable == 'otherkids' ~ 'Other Languages'\n      ),\n    labels = fct_reorder(labels, estimate)\n  ) %&gt;% # Overwrite the variable column of langs_by_boro with a function that tells R to order the variable column by the estimate column when it gets plotted (like a sort). If I wanted it ordered in the other direction I would put estimate inside of desc().\n  filter(variable != 'totalkids')\n\nProduce a horizontal bar chart\n\n# Note - this builds progressively; you can run all the code before any + sign as a step toward the final result.\n\nbarchart &lt;- ggplot(langs_by_boro_for_graphing) +\n  aes(x = estimate, y = labels) +\n  geom_col() +\n  scale_x_continuous(labels = scales::comma) + # format count labels with commas and thousands\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank()) +\n  labs(\n    title = 'What languages do NYC kids speak at home?',\n    subtitle = 'Number of children ages 5-17 by the language spoken at home',\n    x = NULL,\n    y = NULL,\n    caption = \"Source: Census American Community Survey 2020 5-Year Estimates, Table B16007\"\n  )\n\nbarchart\n\n\n\n\n\n\n\n\nNow let’s facet it by borough\n\nbarchart +\n  facet_wrap( ~ name, ncol = 1)",
    "crumbs": [
      "Learning R",
      "12. ggplot"
    ]
  },
  {
    "objectID": "12.ggplot.html#additional-resources",
    "href": "12.ggplot.html#additional-resources",
    "title": "12. ggplot",
    "section": "Additional Resources",
    "text": "Additional Resources\nThis guide from the Urban Institute has a number of helpful tutorials for how to create a wide number of graphs with ggplot.",
    "crumbs": [
      "Learning R",
      "12. ggplot"
    ]
  },
  {
    "objectID": "11.pivoting.html#wide-and-long-data",
    "href": "11.pivoting.html#wide-and-long-data",
    "title": "11. Pivoting",
    "section": "“Wide” and “Long” Data",
    "text": "“Wide” and “Long” Data\nA dataset can be written as wide and long. A wide format contains values that do not repeat in the first column, whereas long format contains values that do repeat in the first column. Imagine keeping statistics for a basket ball game. You could do it two ways:\n\nLong data is tidy - Every column is a variable Every row is an observation. Every cell is a single value.\nWe can use pivoting to convert data between wide and long formats. Here’s a visual for what pivoting does.",
    "crumbs": [
      "Learning R",
      "11. Pivoting"
    ]
  },
  {
    "objectID": "11.pivoting.html#pivoting-longer",
    "href": "11.pivoting.html#pivoting-longer",
    "title": "11. Pivoting",
    "section": "Pivoting Longer",
    "text": "Pivoting Longer\nFor this demo, we want to make a timeline of rent stabilized units in NYC.\nSo, let’s start by loading in some data from https://github.com/talos/nyc-stabilization-unit-counts.\nThe dataset we want is here.\nYou can save this file in your project folder and read it in how we’re used to. Or we can read it directly into our environment, a cool feature of read_csv() that lets you pull directly from a weblink.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nrent_stab_raw &lt;- read_csv(\"https://taxbillsnyc.s3.amazonaws.com/joined.csv\")\n\nRows: 46461 Columns: 61\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (27): borough, 2007est, 2008est, 2009est, 2009dhcr, 2009abat, 2010est, 2...\ndbl (25): ucbbl, 2007uc, 2008uc, 2009uc, 2010uc, 2011uc, 2012uc, 2013uc, 201...\nlgl  (9): 2007dhcr, 2007abat, 2008dhcr, 2008abat, 2010dhcr, 2014dhcr, 2015dh...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis project scraped data from PDFs of property tax documents to get estimates for rent stabilized units counts in buildings across NYC. You can read up on the various field names at the Github project page:\nhttps://github.com/talos/nyc-stabilization-unit-counts#user-content-data-usage.\nFor this demo, we only want to look at rent stabilized unit counts, which according to the Github doccumentation corresponds to column names that end in “uc”. Let’s also grab BBL (which is a unique identifier for NYC buildings) and Borough while we’re at it:\n\nrent_stab &lt;- rent_stab_raw %&gt;% select(borough, ucbbl, ends_with(\"uc\"))\n\n# starts_with(…) and ends_with(…) are neat selector functions to help you \n# grab names that fit a certain pattern \n\nAnnoyingly, the data separates unit counts for different years into different columns… to make a timeline, we need all of the yearly data to be stored in one column.\nWe can use the pivot_longer function included in tidyverse to transform our data accordingly. Here is how we apply the `pivot_longer` function to our data:\n\nrs_long &lt;- rent_stab %&gt;% \n  pivot_longer(\n    ends_with(\"uc\"),  # The multiple column names we want to mush into one column\n    names_to = \"year\", # The title for the new column of names we're generating\n    values_to = \"units\" # The title for the new column of values we're generating\n  )\n\nNow we have data that is “tidy” there is one row for each year for each building. So each observation is a bbl-year pair.",
    "crumbs": [
      "Learning R",
      "11. Pivoting"
    ]
  },
  {
    "objectID": "11.pivoting.html#pivoting-wider",
    "href": "11.pivoting.html#pivoting-wider",
    "title": "11. Pivoting",
    "section": "Pivoting Wider",
    "text": "Pivoting Wider\nAdditionally, you may have data that is in this “long” format and wish to transform it into the “wide” format we are used to. Luckily, there is an analogous function called `pivot_wider` that does just that:\n\nrs_wide &lt;- rs_long %&gt;%\n  pivot_wider(\n  names_from = year, # The current column containing our future column names\n  values_from = units # The current column containing the values for our future columns\n  )",
    "crumbs": [
      "Learning R",
      "11. Pivoting"
    ]
  },
  {
    "objectID": "13.stringr.html#what-is-a-string",
    "href": "13.stringr.html#what-is-a-string",
    "title": "13. Working with Strings",
    "section": "What is a string?",
    "text": "What is a string?\nStrings are groups of characters. They can form words, sentences, addresses, categories, or represent other data. Working with strings is going to be essential to cleaning data. Often times we get string and character data that is manually inputted and messy. We can use string modifiers to clean it up. Think of address data for example. What if addresses are misspelled or inconsistent in how they refer to street names?\nIn this lesson we’ll learn a number of useful functions for manipulating and rewriting strings, using some base R, tidyverse, and a package called stringr.",
    "crumbs": [
      "Learning R",
      "13. Working with Strings"
    ]
  },
  {
    "objectID": "13.stringr.html#separate-paste",
    "href": "13.stringr.html#separate-paste",
    "title": "13. Working with Strings",
    "section": "Separate & Paste",
    "text": "Separate & Paste\nLet’s read in a clean dataframe for film permits from the open data portal.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nfilmpermits &lt;- read_csv(\"Film_Permits.csv\", \n                        col_types = cols(EventID = col_character())) %&gt;%\n  clean_names()\n\nWe can use separate() to split up long string columns and paste() to concatenate strings into new columns\n\nstreet_closures &lt;- filmpermits %&gt;% \n  separate_rows(parking_held, sep = \",\") %&gt;% #this makes a new row for each street closure listed\n  separate(parking_held, into = c(\"street\", \"cross_street1\", \"cross_street2\"), sep = \"between|and\") %&gt;% #this separates the string into three different columns\n  mutate_at(.vars = c(\"street\", \"cross_street1\", \"cross_street2\", \"borough\", \"country\"), .funs = ~tolower(.)) %&gt;% # a .fns argument takes a function like tolower, which makes all the characters lowercase. start the function with ~ and subsitite . for the argument of the nested function\n  mutate(intersection1_address = paste(street, \"and\", cross_street1, borough, country), #paste concatenates strings to create, in this case an address of an intersection\n         intersection2_address = paste(street, \"and\", cross_street1, borough, country))",
    "crumbs": [
      "Learning R",
      "13. Working with Strings"
    ]
  },
  {
    "objectID": "13.stringr.html#stringr",
    "href": "13.stringr.html#stringr",
    "title": "13. Working with Strings",
    "section": "StringR",
    "text": "StringR\nStringR is a package with a number of handy functions. All of them take a string as the first argument, and return the result of your inquiry. There’s tons of possibilities and uses, here’s few examples.\n\n\nThe spacing for the strings in the street columns is weird, I can use str_trim to get rid of white space\n\nlibrary(stringr)\n\nstreet_closures %&gt;% head() %&gt;% .$street #here I am just printing the first 10 rows of the variable street\n\n[1] \"commerce street \"   \"  seabring street \" \"amsterdam avenue \" \n[4] \"eagle street \"      \"  west street \"     \"  freeman street \" \n\nstreet_closures %&gt;% \n  mutate(street = str_trim(street, side = \"both\")) %&gt;% \n  head() %&gt;% #here I am just printing the first 10 rows of the variable street that I overwrote\n  .$street\n\n[1] \"commerce street\"  \"seabring street\"  \"amsterdam avenue\" \"eagle street\"    \n[5] \"west street\"      \"freeman street\"  \n\n\nOr perhaps I want to know the total closures on 14th street, but I don’t care if it’s West 14th street or East 14th street, or I’m worried that sometimes it’s in the data as “street” and other times as “st” - making it hard to do an exact match. str_detect finds matches of certain patterns. We can pair it with conditional logic to create a variable that flags if the closure was on the street we care about.\n\nstreet_closures %&gt;% \n  filter(borough == \"manhattan\") %&gt;% \n  mutate(fourteenth_street = if_else(str_detect(street, \"14 |14th\"), T, F)) %&gt;% #use these with conditional logic!\n  count(fourteenth_street) #count how many times!\n\n# A tibble: 2 × 2\n  fourteenth_street     n\n  &lt;lgl&gt;             &lt;int&gt;\n1 FALSE              5110\n2 TRUE                 17\n\n\nOr use stringr to change data entry inconsistencies!\n\nstreet_replace &lt;- street_closures %&gt;% \n  mutate(street = str_replace(street, \"14 \", \"14th\")) \n\nWriting regular expressions, the complex notation you can use to return parts of strings is a great example of something you can use AI for!\n\n\n# Define the regular expression pattern\npattern &lt;- \"^(.*?)(?=\\\\b(?:street|boulevard|avenue|st|blvd|ave|rd|road|circle|cr)\\\\b)\"\n\n# Use str_match to extract the matched part\nstr_extract(head(street_closures)$street, pattern)\n\n[1] \"commerce \"   \"  seabring \" \"amsterdam \"  \"eagle \"      \"  west \"    \n[6] \"  freeman \"",
    "crumbs": [
      "Learning R",
      "13. Working with Strings"
    ]
  },
  {
    "objectID": "17.sf.html",
    "href": "17.sf.html",
    "title": "17. Spatial Features",
    "section": "",
    "text": "The Spatial features package allows us to read in spatial data into R and transform it. Let’s get a spatial dataset on affordable housing from the NYC open data portal.\n\naff_hsg &lt;- read_csv(\"https://data.cityofnewyork.us/resource/hg8x-zxpr.csv?$limit=10000\")\n\nRows: 7637 Columns: 41\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (9): project_name, house_number, street_name, borough, community_board...\ndbl  (29): project_id, building_id, postcode, bbl, bin, council_district, ce...\ndttm  (3): project_start_date, project_completion_date, building_completion_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWith SF, I can take tabular data and make it into a spatial dataset. In this case I have coordinates that I turn into points us st_as_sf\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\naff_hsg_sf &lt;- aff_hsg %&gt;% \n  clean_names() %&gt;% \n  filter(!is.na(longitude)) %&gt;%  #remove properties with missing coordinates\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 2263) #be careful! x = longitude, y = latitude\n\nNow that this is spatial, I can map it! geom_sf is the ggplot function that maps spatial objects. It works much like any other ggplot.\n\nggplot()+\n  geom_sf(aff_hsg_sf, mapping = aes())\n\n\n\n\n\n\n\n\nI can use programming for styling, too.\n\nggplot()+\n  geom_sf(aff_hsg_sf,\n          mapping = aes(size = all_counted_units,\n                        color = all_counted_units),\n          alpha = 0.25)\n\n\n\n\n\n\n\n\nBut that doesn’t look very good, let’s add some other layers. I can read in CDs directly as a shapefile using the geojson version from the Open data portal. This map is a long way from done, but now it has a semblance of a basemap.\n\ncd_sf &lt;- read_sf(\"https://data.cityofnewyork.us/resource/jp9i-3b7y.geojson\") %&gt;% \n  st_set_crs(st_crs(aff_hsg_sf)) # here I am setting the crs of the new data to the crs of the point data I already have\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nggplot()+\n  geom_sf(cd_sf,\n          mapping = aes())+ #this layer will be on bottom!\n  geom_sf(aff_hsg_sf,\n          mapping = aes(size = all_counted_units,\n                        color = all_counted_units),\n          alpha = 0.25) #this layer will be on top!\n\n\n\n\n\n\n\n          #alpha changes the opacity of the dots\n\nI can also summarize data like I would in a tabular dataset\n\naff_hsg_sum &lt;- aff_hsg %&gt;% \n  group_by(community_board) %&gt;% \n  summarize(total_affordable_units = sum(all_counted_units, na.rm = T))\n\nAnd then I could create a choropleth with the summarized data, now that I have the nta shapes (just a simple join!)\n\ncd_aff_sum &lt;- cd_sf %&gt;% \n  mutate(community_board = case_when(\n    str_sub(boro_cd, 1, 1) == \"1\" ~ paste0(\"MN-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"2\" ~ paste0(\"BX-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"3\" ~ paste0(\"BK-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"4\" ~ paste0(\"QN-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"5\" ~ paste0(\"SI-\",str_sub(boro_cd, 2,3)),\n  )) %&gt;%  #take the first character of boro_cd, replace it with the boro abbreviation, add the community board number\n  left_join(aff_hsg_sum, by = \"community_board\")\n\nI can map that, now as a choropleth\n\nggplot()+\n  geom_sf(cd_aff_sum,\n          mapping = aes(fill = total_affordable_units))\n\n\n\n\n\n\n\n\nI could also match it to other data, and write it out to a shapefile to use in GIS\n\nlibrary(tidycensus)\n\ncensus_stats &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(population = \"B01003_001\",\n                med_income = \"B07011_001\"),\n  year = 2021,\n  state = \"NY\",\n  county = c(\"Bronx\", \"New York\", \"Kings\", \"Queens\", \"Richmond\"),\n  output = \"wide\"\n) %&gt;% \n  clean_names()\n\nGetting data from the 2017-2021 5-year ACS\n\n\nI need a crosswalk to go from census tracts to community districts. Open data has one!\n\ncdtas_tracts &lt;- read_csv(\"https://data.cityofnewyork.us/resource/hm78-6dwm.csv?$limit=10000\",\n                         col_types = cols(geoid = col_character()))\n\ncdta_stats &lt;- census_stats %&gt;%\n  left_join(cdtas_tracts, by = \"geoid\") %&gt;% \n  group_by(cdtacode, cdtaname) %&gt;% \n  summarize(total_pop = sum(population_e, na.rm = T),\n            avg_med_inc = mean(med_income_e, na.rm = T)) %&gt;% \n  mutate()\n\n`summarise()` has grouped output by 'cdtacode'. You can override using the\n`.groups` argument.\n\n\nNow I can join! With everything in the same data frame I can write it out to read into spatial software, or I can visualize it\n\ncd_aff_stats &lt;- cd_aff_sum %&gt;% \n  mutate(cdtacode = str_replace(community_board, \"-\", \"\")) %&gt;% \n  left_join(cdta_stats) %&gt;% \n  mutate(aff_units_person = total_affordable_units/total_pop)\n\nJoining with `by = join_by(cdtacode)`\n\n\n\nst_write(cd_aff_stats,\n         \"output/cd_aff_hsg.shp\",\n         append = F)\n\nWarning in abbreviate_shapefile_names(obj): Field names abbreviated for ESRI\nShapefile driver\n\n\nDeleting layer `cd_aff_hsg' using driver `ESRI Shapefile'\nWriting layer `cd_aff_hsg' to data source \n  `output/cd_aff_hsg.shp' using driver `ESRI Shapefile'\nWriting 71 features with 10 fields and geometry type Multi Polygon.\n\n\nI can map affordable units per person (normalized!)\n\nggplot()+\n  geom_sf(cd_aff_stats,\n          mapping = aes(fill = aff_units_person))\n\n\n\n\n\n\n\n\nWhat if I wanted to map both both the # of units per cd and the median income?\nI can use one of sf’s spatial operators and to take a centroid and map points\n\npoints_aff_cd &lt;- cd_aff_stats %&gt;% \n  select(total_affordable_units, cdtacode) %&gt;% \n  st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nAnd overlay on a choropleth. You notice that a lot of the buildings are built in low income areas!\n\nggplot()+\n  geom_sf(cd_aff_stats,\n          mapping = aes(fill = avg_med_inc))+\n  geom_sf(points_aff_cd,\n          mapping = aes(size = total_affordable_units),\n          color = \"pink\",\n          alpha = 0.5)\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_sf()`).\n\n\n\n\n\n\n\n\n\nI could write out this ggplot to edit in vector graphics\n\nggsave(\"output/aff_hsg_inc_nyc.svg\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_sf()`).\n\n\nWhat if I didn’t have a crosswalk and needed to match points to the community district? I can do a spatial join to find what cd all the points fall into.\n\npoints_polygons_join &lt;- st_intersection(aff_hsg_sf, cd_sf)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nI can then use summarize() to count points in polygons - or do more advanced summaries\n\npoints_polygons_join %&gt;% \n  as.data.frame() %&gt;% #summarize works faster without the spatial features attached\n  group_by(boro_cd) %&gt;% \n  summarize(points_in_polygon = n(),\n            units_per_cd = sum(all_counted_units, na.rm = T))\n\n# A tibble: 59 × 3\n   boro_cd points_in_polygon units_per_cd\n   &lt;chr&gt;               &lt;int&gt;        &lt;dbl&gt;\n 1 101                     5          300\n 2 102                     9          639\n 3 103                   142         8519\n 4 104                    69         6963\n 5 105                    18         1595\n 6 106                    79         6636\n 7 107                    84         3950\n 8 108                    19         1587\n 9 109                   117         2781\n10 110                   357        10381\n# ℹ 49 more rows\n\n\nWhat if I wanted to find the proportion of affordable housing in the flood zone? I can do this with an intersects and mutate - I don’t even need to join the two datasets!\n\n# floodplain_2020s_100y &lt;- read_sf(\"https://data.cityofnewyork.us/resource/inra-wqx3.geojson?$limit=10000\") %&gt;% \n#   st_make_valid() %&gt;%  #this magic function repairs any invalid geometries\n#   st_union() %&gt;% #here I make the entire floodplain into one big shape\n#   st_set_crs(st_crs(points_polygons_join)) #and set it to the same crs as my points\n# \n# points_polygons_join %&gt;% \n#   mutate(fplain_2020s_100y = lengths(st_intersects(.,floodplain_2020s_100y))) %&gt;%  #st_intersects returns a list of the shapes it intersects with - if its 0 it didn't intersect, if its 1 it did!\n#   as.data.frame() %&gt;% \n#   group_by(fplain_2020s_100y) %&gt;% \n#   summarize(count = n(),\n#             units = sum(all_counted_units, na.rm = T))\n\nMany more spatial operations available on the cheat sheets",
    "crumbs": [
      "Learning R",
      "17. Spatial Features"
    ]
  },
  {
    "objectID": "17.sf.html#video-tutorial",
    "href": "17.sf.html#video-tutorial",
    "title": "17. Spatial Features",
    "section": "",
    "text": "The Spatial features package allows us to read in spatial data into R and transform it. Let’s get a spatial dataset on affordable housing from the NYC open data portal.\n\naff_hsg &lt;- read_csv(\"https://data.cityofnewyork.us/resource/hg8x-zxpr.csv?$limit=10000\")\n\nRows: 7637 Columns: 41\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (9): project_name, house_number, street_name, borough, community_board...\ndbl  (29): project_id, building_id, postcode, bbl, bin, council_district, ce...\ndttm  (3): project_start_date, project_completion_date, building_completion_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWith SF, I can take tabular data and make it into a spatial dataset. In this case I have coordinates that I turn into points us st_as_sf\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\naff_hsg_sf &lt;- aff_hsg %&gt;% \n  clean_names() %&gt;% \n  filter(!is.na(longitude)) %&gt;%  #remove properties with missing coordinates\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 2263) #be careful! x = longitude, y = latitude\n\nNow that this is spatial, I can map it! geom_sf is the ggplot function that maps spatial objects. It works much like any other ggplot.\n\nggplot()+\n  geom_sf(aff_hsg_sf, mapping = aes())\n\n\n\n\n\n\n\n\nI can use programming for styling, too.\n\nggplot()+\n  geom_sf(aff_hsg_sf,\n          mapping = aes(size = all_counted_units,\n                        color = all_counted_units),\n          alpha = 0.25)\n\n\n\n\n\n\n\n\nBut that doesn’t look very good, let’s add some other layers. I can read in CDs directly as a shapefile using the geojson version from the Open data portal. This map is a long way from done, but now it has a semblance of a basemap.\n\ncd_sf &lt;- read_sf(\"https://data.cityofnewyork.us/resource/jp9i-3b7y.geojson\") %&gt;% \n  st_set_crs(st_crs(aff_hsg_sf)) # here I am setting the crs of the new data to the crs of the point data I already have\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nggplot()+\n  geom_sf(cd_sf,\n          mapping = aes())+ #this layer will be on bottom!\n  geom_sf(aff_hsg_sf,\n          mapping = aes(size = all_counted_units,\n                        color = all_counted_units),\n          alpha = 0.25) #this layer will be on top!\n\n\n\n\n\n\n\n          #alpha changes the opacity of the dots\n\nI can also summarize data like I would in a tabular dataset\n\naff_hsg_sum &lt;- aff_hsg %&gt;% \n  group_by(community_board) %&gt;% \n  summarize(total_affordable_units = sum(all_counted_units, na.rm = T))\n\nAnd then I could create a choropleth with the summarized data, now that I have the nta shapes (just a simple join!)\n\ncd_aff_sum &lt;- cd_sf %&gt;% \n  mutate(community_board = case_when(\n    str_sub(boro_cd, 1, 1) == \"1\" ~ paste0(\"MN-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"2\" ~ paste0(\"BX-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"3\" ~ paste0(\"BK-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"4\" ~ paste0(\"QN-\",str_sub(boro_cd, 2,3)),\n    str_sub(boro_cd, 1, 1) == \"5\" ~ paste0(\"SI-\",str_sub(boro_cd, 2,3)),\n  )) %&gt;%  #take the first character of boro_cd, replace it with the boro abbreviation, add the community board number\n  left_join(aff_hsg_sum, by = \"community_board\")\n\nI can map that, now as a choropleth\n\nggplot()+\n  geom_sf(cd_aff_sum,\n          mapping = aes(fill = total_affordable_units))\n\n\n\n\n\n\n\n\nI could also match it to other data, and write it out to a shapefile to use in GIS\n\nlibrary(tidycensus)\n\ncensus_stats &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(population = \"B01003_001\",\n                med_income = \"B07011_001\"),\n  year = 2021,\n  state = \"NY\",\n  county = c(\"Bronx\", \"New York\", \"Kings\", \"Queens\", \"Richmond\"),\n  output = \"wide\"\n) %&gt;% \n  clean_names()\n\nGetting data from the 2017-2021 5-year ACS\n\n\nI need a crosswalk to go from census tracts to community districts. Open data has one!\n\ncdtas_tracts &lt;- read_csv(\"https://data.cityofnewyork.us/resource/hm78-6dwm.csv?$limit=10000\",\n                         col_types = cols(geoid = col_character()))\n\ncdta_stats &lt;- census_stats %&gt;%\n  left_join(cdtas_tracts, by = \"geoid\") %&gt;% \n  group_by(cdtacode, cdtaname) %&gt;% \n  summarize(total_pop = sum(population_e, na.rm = T),\n            avg_med_inc = mean(med_income_e, na.rm = T)) %&gt;% \n  mutate()\n\n`summarise()` has grouped output by 'cdtacode'. You can override using the\n`.groups` argument.\n\n\nNow I can join! With everything in the same data frame I can write it out to read into spatial software, or I can visualize it\n\ncd_aff_stats &lt;- cd_aff_sum %&gt;% \n  mutate(cdtacode = str_replace(community_board, \"-\", \"\")) %&gt;% \n  left_join(cdta_stats) %&gt;% \n  mutate(aff_units_person = total_affordable_units/total_pop)\n\nJoining with `by = join_by(cdtacode)`\n\n\n\nst_write(cd_aff_stats,\n         \"output/cd_aff_hsg.shp\",\n         append = F)\n\nWarning in abbreviate_shapefile_names(obj): Field names abbreviated for ESRI\nShapefile driver\n\n\nDeleting layer `cd_aff_hsg' using driver `ESRI Shapefile'\nWriting layer `cd_aff_hsg' to data source \n  `output/cd_aff_hsg.shp' using driver `ESRI Shapefile'\nWriting 71 features with 10 fields and geometry type Multi Polygon.\n\n\nI can map affordable units per person (normalized!)\n\nggplot()+\n  geom_sf(cd_aff_stats,\n          mapping = aes(fill = aff_units_person))\n\n\n\n\n\n\n\n\nWhat if I wanted to map both both the # of units per cd and the median income?\nI can use one of sf’s spatial operators and to take a centroid and map points\n\npoints_aff_cd &lt;- cd_aff_stats %&gt;% \n  select(total_affordable_units, cdtacode) %&gt;% \n  st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nAnd overlay on a choropleth. You notice that a lot of the buildings are built in low income areas!\n\nggplot()+\n  geom_sf(cd_aff_stats,\n          mapping = aes(fill = avg_med_inc))+\n  geom_sf(points_aff_cd,\n          mapping = aes(size = total_affordable_units),\n          color = \"pink\",\n          alpha = 0.5)\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_sf()`).\n\n\n\n\n\n\n\n\n\nI could write out this ggplot to edit in vector graphics\n\nggsave(\"output/aff_hsg_inc_nyc.svg\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_sf()`).\n\n\nWhat if I didn’t have a crosswalk and needed to match points to the community district? I can do a spatial join to find what cd all the points fall into.\n\npoints_polygons_join &lt;- st_intersection(aff_hsg_sf, cd_sf)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nI can then use summarize() to count points in polygons - or do more advanced summaries\n\npoints_polygons_join %&gt;% \n  as.data.frame() %&gt;% #summarize works faster without the spatial features attached\n  group_by(boro_cd) %&gt;% \n  summarize(points_in_polygon = n(),\n            units_per_cd = sum(all_counted_units, na.rm = T))\n\n# A tibble: 59 × 3\n   boro_cd points_in_polygon units_per_cd\n   &lt;chr&gt;               &lt;int&gt;        &lt;dbl&gt;\n 1 101                     5          300\n 2 102                     9          639\n 3 103                   142         8519\n 4 104                    69         6963\n 5 105                    18         1595\n 6 106                    79         6636\n 7 107                    84         3950\n 8 108                    19         1587\n 9 109                   117         2781\n10 110                   357        10381\n# ℹ 49 more rows\n\n\nWhat if I wanted to find the proportion of affordable housing in the flood zone? I can do this with an intersects and mutate - I don’t even need to join the two datasets!\n\n# floodplain_2020s_100y &lt;- read_sf(\"https://data.cityofnewyork.us/resource/inra-wqx3.geojson?$limit=10000\") %&gt;% \n#   st_make_valid() %&gt;%  #this magic function repairs any invalid geometries\n#   st_union() %&gt;% #here I make the entire floodplain into one big shape\n#   st_set_crs(st_crs(points_polygons_join)) #and set it to the same crs as my points\n# \n# points_polygons_join %&gt;% \n#   mutate(fplain_2020s_100y = lengths(st_intersects(.,floodplain_2020s_100y))) %&gt;%  #st_intersects returns a list of the shapes it intersects with - if its 0 it didn't intersect, if its 1 it did!\n#   as.data.frame() %&gt;% \n#   group_by(fplain_2020s_100y) %&gt;% \n#   summarize(count = n(),\n#             units = sum(all_counted_units, na.rm = T))\n\nMany more spatial operations available on the cheat sheets",
    "crumbs": [
      "Learning R",
      "17. Spatial Features"
    ]
  },
  {
    "objectID": "34.isochrones_areas.html",
    "href": "34.isochrones_areas.html",
    "title": "34. Isochrones and Calculating Area",
    "section": "",
    "text": "Use the TravelTime plugin to create isochrone polygons, which show the area that is within a certain travel time to a point by a specified mode of transit (in this example, 5 minutes walking distance from a subway station). Apply merge, dissolve, and clip geoprocessing tools. Calculate and divide the area of the resulting isochrone by the area of the larger community district to obtain the percentage of the community district that is within a 5 minute walk to a subway station.\nTimestamps (Click a link to start that segment in a new window)\n0:00 - 2:57: Create a shapefile of a single community district and clip subways to that community district\n2:58 - 12:19: Create an isochrone\n12:20 - 13:32: Additional analysis possibilities\n13:32 - 16:42: Calculate the area of the isochrone as a percentage of the community district",
    "crumbs": [
      "QGIS",
      "34. Isochrones and Calculating Area"
    ]
  },
  {
    "objectID": "34.isochrones_areas.html#video-tutorial",
    "href": "34.isochrones_areas.html#video-tutorial",
    "title": "34. Isochrones and Calculating Area",
    "section": "",
    "text": "Use the TravelTime plugin to create isochrone polygons, which show the area that is within a certain travel time to a point by a specified mode of transit (in this example, 5 minutes walking distance from a subway station). Apply merge, dissolve, and clip geoprocessing tools. Calculate and divide the area of the resulting isochrone by the area of the larger community district to obtain the percentage of the community district that is within a 5 minute walk to a subway station.\nTimestamps (Click a link to start that segment in a new window)\n0:00 - 2:57: Create a shapefile of a single community district and clip subways to that community district\n2:58 - 12:19: Create an isochrone\n12:20 - 13:32: Additional analysis possibilities\n13:32 - 16:42: Calculate the area of the isochrone as a percentage of the community district",
    "crumbs": [
      "QGIS",
      "34. Isochrones and Calculating Area"
    ]
  },
  {
    "objectID": "34.isochrones_areas.html#data-downloads",
    "href": "34.isochrones_areas.html#data-downloads",
    "title": "34. Isochrones and Calculating Area",
    "section": "Data downloads",
    "text": "Data downloads\nCommunity Districts\nSubway stations:\n\nMTA subway stations shapefile via NYS\nSaved copy",
    "crumbs": [
      "QGIS",
      "34. Isochrones and Calculating Area"
    ]
  },
  {
    "objectID": "29.select.html",
    "href": "29.select.html",
    "title": "29. Selecting features",
    "section": "",
    "text": "Select features manually with the attribute table or directly on your map, as well as select features by value and with mathematic functions",
    "crumbs": [
      "QGIS",
      "29. Selecting features"
    ]
  },
  {
    "objectID": "29.select.html#video-tutorial",
    "href": "29.select.html#video-tutorial",
    "title": "29. Selecting features",
    "section": "",
    "text": "Select features manually with the attribute table or directly on your map, as well as select features by value and with mathematic functions",
    "crumbs": [
      "QGIS",
      "29. Selecting features"
    ]
  },
  {
    "objectID": "29.select.html#data-downloads",
    "href": "29.select.html#data-downloads",
    "title": "29. Selecting features",
    "section": "Data downloads",
    "text": "Data downloads\nBorough boundaries\nNYC subway stations (Saved copy)",
    "crumbs": [
      "QGIS",
      "29. Selecting features"
    ]
  },
  {
    "objectID": "8.groupby_summarize.html#why-summarize-data",
    "href": "8.groupby_summarize.html#why-summarize-data",
    "title": "8. Group By and Summarize",
    "section": "Why summarize data?",
    "text": "Why summarize data?\nHaving clean data is great, but when working with large datasets we are often looking for summary statistics to let us compare different groups. group_by and summarize, often used together in a pipe, are a powerful combo for generating statistics at the group level.\nSummarizing allows us to compare means, medians, or top values based on different categories. It can also be a helpful data cleaning tool, depending on the level of observations in the data.",
    "crumbs": [
      "Learning R",
      "8. Group By and Summarize"
    ]
  },
  {
    "objectID": "8.groupby_summarize.html#summarize",
    "href": "8.groupby_summarize.html#summarize",
    "title": "8. Group By and Summarize",
    "section": "Summarize",
    "text": "Summarize\nSummarize takes a data frame and a ... list of new variables to generate.\nLet’s grab our code to read in the clean dataframe again.\n\nfhv_clean &lt;- read_csv(file = \"For_Hire_Vehicles__FHV__-_Active.csv\") %&gt;% \n  clean_names() %&gt;% \n  rename(hybrid = veh)\n\nNote - summarize is different than summary - a helpful function that provides the basic stats of all variables - a good first step when looking at a new data set\n\nsummary(fhv_clean)\n\n    active          vehicle_license_number     name          \n Length:98318       Length:98318           Length:98318      \n Class :character   Class :character       Class :character  \n Mode  :character   Mode  :character       Mode  :character  \n                                                             \n                                                             \n                                                             \n license_type       expiration_date    permit_license_number\n Length:98318       Length:98318       Length:98318         \n Class :character   Class :character   Class :character     \n Mode  :character   Mode  :character   Mode  :character     \n                                                            \n                                                            \n                                                            \n dmv_license_plate_number vehicle_vin_number wheelchair_accessible\n Length:98318             Length:98318       Length:98318         \n Class :character         Class :character   Class :character     \n Mode  :character         Mode  :character   Mode  :character     \n                                                                  \n                                                                  \n                                                                  \n certification_date hack_up_date        vehicle_year  base_number       \n Length:98318       Length:98318       Min.   :1949   Length:98318      \n Class :character   Class :character   1st Qu.:2016   Class :character  \n Mode  :character   Mode  :character   Median :2018   Mode  :character  \n                                       Mean   :2018                     \n                                       3rd Qu.:2021                     \n                                       Max.   :5015                     \n  base_name          base_type            hybrid          base_telephone_number\n Length:98318       Length:98318       Length:98318       Length:98318         \n Class :character   Class :character   Class :character   Class :character     \n Mode  :character   Mode  :character   Mode  :character   Mode  :character     \n                                                                               \n                                                                               \n                                                                               \n   website          base_address          reason          order_date    \n Length:98318       Length:98318       Length:98318       Mode:logical  \n Class :character   Class :character   Class :character   NA's:98318    \n Mode  :character   Mode  :character   Mode  :character                 \n                                                                        \n                                                                        \n                                                                        \n last_date_updated  last_time_updated\n Length:98318       Length:98318     \n Class :character   Class1:hms       \n Mode  :character   Class2:difftime  \n                    Mode  :numeric   \n                                     \n                                     \n\n\nWith a basic application of summarize, we take all the rows of the dataframe and turn them into one row of data. Here we use mean() to get the average\n\nfhv_clean %&gt;% \n  summarize(average_year = mean(vehicle_year))\n\n# A tibble: 1 × 1\n  average_year\n         &lt;dbl&gt;\n1        2018.\n\n#careful - NAs are sticky so mean() will return NA if there are any missing values. Use na.rm = T to exclude missing in calculating the average\n\nfhv_clean %&gt;% \n  summarize(average_year = mean(vehicle_year, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  average_year\n         &lt;dbl&gt;\n1        2018.\n\n\nYou can summarize multiple variables (...) at once. Look at summarize documentation for the full list of operations you can summarize with.\n\nfhv_clean %&gt;% \n  summarize(total_cars = n(), #n() just counts the number of rows in the group\n            average_year = mean(vehicle_year, na.rm = TRUE),\n            median_year = median(vehicle_year, na.rm = T))\n\n# A tibble: 1 × 3\n  total_cars average_year median_year\n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1      98318        2018.        2018\n\n\nWhen using summarize it defaults to one big group - all the data is summarized. To summarize by group we can add group_by()",
    "crumbs": [
      "Learning R",
      "8. Group By and Summarize"
    ]
  },
  {
    "objectID": "8.groupby_summarize.html#group_by",
    "href": "8.groupby_summarize.html#group_by",
    "title": "8. Group By and Summarize",
    "section": "Group_by",
    "text": "Group_by\nWhere summarize becomes really powerful is pairing it with group_by\n\nfhv_clean %&gt;% \n  group_by(hybrid) %&gt;% \n  summarize(number_cars = n(),\n            mean_year = mean(vehicle_year, na.rm = T))\n\n# A tibble: 9 × 3\n  hybrid number_cars mean_year\n  &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt;\n1 BEV           2267     2022.\n2 CNG              1     2020 \n3 DSE              1     2019 \n4 HYB           3267     2019.\n5 N                1     2016 \n6 NON              1     2012 \n7 STR             33     2017.\n8 WAV           6016     2019.\n9 &lt;NA&gt;         86731     2018.\n\n\nOften we are grouping on variables of interest and summarizing across that variable. Take the variable we made last time, for example\n\nfhv_type_summary &lt;- fhv_clean %&gt;% \n  mutate(\n    ride_type = case_when(\n      base_name == \"UBER USA, LLC\" & base_type == \"BLACK-CAR\" ~ \"BLACK CAR RIDESHARE\",\n      base_name != \"UBER USA, LLC\" & base_type == \"BLACK-CAR\" ~ \"BLACK CAR NON-RIDESHARE\",\n      TRUE ~ base_type #if it doesn't meet either condition, return the base_type\n    )) %&gt;% \n  group_by(ride_type) %&gt;% #group by the variable we just created!\n  summarize(no_cars = n(),\n            average_year = mean(vehicle_year, na.rm = T))\n\nfhv_type_summary\n\n# A tibble: 4 × 3\n  ride_type               no_cars average_year\n  &lt;chr&gt;                     &lt;int&gt;        &lt;dbl&gt;\n1 BLACK CAR NON-RIDESHARE   16225        2018.\n2 BLACK CAR RIDESHARE       76710        2018.\n3 LIVERY                     3652        2015.\n4 LUXURY                     1731        2020.\n\n\nHere we can see some interesting trends start to emerge, like how the livery cars tend to be the oldest and the luxury cars tend to be newer.\nWe can also group_by multiple variables, or group by logical expressions.\n\nfhv_clean %&gt;% \n  group_by(hybrid, base_type) %&gt;% \n  summarize(total_cars = n())\n\n`summarise()` has grouped output by 'hybrid'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 18 × 3\n# Groups:   hybrid [9]\n   hybrid base_type total_cars\n   &lt;chr&gt;  &lt;chr&gt;          &lt;int&gt;\n 1 BEV    BLACK-CAR       2232\n 2 BEV    LIVERY            17\n 3 BEV    LUXURY            18\n 4 CNG    BLACK-CAR          1\n 5 DSE    BLACK-CAR          1\n 6 HYB    BLACK-CAR       3009\n 7 HYB    LIVERY           174\n 8 HYB    LUXURY            84\n 9 N      BLACK-CAR          1\n10 NON    BLACK-CAR          1\n11 STR    BLACK-CAR         18\n12 STR    LUXURY            15\n13 WAV    BLACK-CAR       5904\n14 WAV    LIVERY            96\n15 WAV    LUXURY            16\n16 &lt;NA&gt;   BLACK-CAR      81768\n17 &lt;NA&gt;   LIVERY          3365\n18 &lt;NA&gt;   LUXURY          1598\n\nfhv_clean %&gt;% \n  group_by(base_type, vehicle_year &gt;= 2000) %&gt;% \n  summarize(total_cars = n())\n\n`summarise()` has grouped output by 'base_type'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   base_type [3]\n  base_type `vehicle_year &gt;= 2000` total_cars\n  &lt;chr&gt;     &lt;lgl&gt;                       &lt;int&gt;\n1 BLACK-CAR TRUE                        92935\n2 LIVERY    TRUE                         3652\n3 LUXURY    FALSE                           5\n4 LUXURY    TRUE                         1726",
    "crumbs": [
      "Learning R",
      "8. Group By and Summarize"
    ]
  },
  {
    "objectID": "8.groupby_summarize.html#group_by-with-mutate",
    "href": "8.groupby_summarize.html#group_by-with-mutate",
    "title": "8. Group By and Summarize",
    "section": "Group_by with mutate",
    "text": "Group_by with mutate\nPair group_by with mutate to create helpful summary level variables without reducing the number of rows in the dataset.\n\nfhv_clean %&gt;% \n  group_by(base_name) %&gt;% \n  mutate(total_by_name = n()) #variable with the total # of cars for each company name\n\n# A tibble: 98,318 × 24\n# Groups:   base_name [771]\n   active vehicle_license_number name               license_type expiration_date\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;          \n 1 YES    5608977                AMERICAN,UNITED,T… FOR HIRE VE… 04/30/2025     \n 2 YES    5645622                RAMA,ILIR          FOR HIRE VE… 09/11/2023     \n 3 YES    5192507                ORDONEZ,ELIAS      FOR HIRE VE… 03/08/2025     \n 4 YES    5378856                RIVERA,ENMA        FOR HIRE VE… 11/12/2024     \n 5 YES    5852121                A/VA,SERVICE,CORP  FOR HIRE VE… 04/11/2024     \n 6 YES    5415237                REYES,JUAN,E       FOR HIRE VE… 10/31/2023     \n 7 YES    5643301                BEGUM,TAZMINUR     FOR HIRE VE… 09/30/2025     \n 8 YES    5701439                GONZALEZALVARADO,L FOR HIRE VE… 06/13/2024     \n 9 YES    5790931                GOMEZ,JOSE,A       FOR HIRE VE… 05/23/2025     \n10 YES    5743759                HOSSAIN,SM,KAMAL   FOR HIRE VE… 12/08/2024     \n# ℹ 98,308 more rows\n# ℹ 19 more variables: permit_license_number &lt;chr&gt;,\n#   dmv_license_plate_number &lt;chr&gt;, vehicle_vin_number &lt;chr&gt;,\n#   wheelchair_accessible &lt;chr&gt;, certification_date &lt;chr&gt;, hack_up_date &lt;chr&gt;,\n#   vehicle_year &lt;dbl&gt;, base_number &lt;chr&gt;, base_name &lt;chr&gt;, base_type &lt;chr&gt;,\n#   hybrid &lt;chr&gt;, base_telephone_number &lt;chr&gt;, website &lt;chr&gt;,\n#   base_address &lt;chr&gt;, reason &lt;chr&gt;, order_date &lt;lgl&gt;, …\n\n\nUse ungroup() to return to normal mutation operations\n\nfhv_clean %&gt;% \n  group_by(base_type) %&gt;% \n  mutate(mean_by_type = mean(vehicle_year, na.rm =T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(above_below_mean = if_else(\n    condition = vehicle_year &gt; mean_by_type,\n    true = \"above mean\",\n    false = \"below mean\"\n  )) %&gt;% \n  count(above_below_mean)\n\n# A tibble: 2 × 2\n  above_below_mean     n\n  &lt;chr&gt;            &lt;int&gt;\n1 above mean       44530\n2 below mean       53788\n\n#this creates a variable to show if this car is above or below the average year for the group",
    "crumbs": [
      "Learning R",
      "8. Group By and Summarize"
    ]
  },
  {
    "objectID": "8.groupby_summarize.html#normalizing-with-groups",
    "href": "8.groupby_summarize.html#normalizing-with-groups",
    "title": "8. Group By and Summarize",
    "section": "Normalizing with Groups",
    "text": "Normalizing with Groups\nGroup by, summarize, and mutate are crucial when comparing geographic areas. You can use group_by to get the proportion within a certain group.\nHere’s a full example of how to normalize within a group, using the data on new york housing authority apartments by borough. We want to find the proportion of NYCHA apartments that are Section 8 (vouchers) in each borough.\n\nnycha &lt;- read_csv('https://data.cityofnewyork.us/resource/evjd-dqpz.csv') %&gt;%\n  clean_names()\n\n# Filter for developments that contain any Section 8 transition apartments\nsection8devs &lt;- nycha %&gt;%\n  filter(number_of_section_8_transition_apartments &gt; 0)\n\n# Get some stats on number of section 8 apts across all developments by borough\nsection8devs_by_boro &lt;- section8devs %&gt;%\n  group_by(borough) %&gt;%\n  summarize(section8apts = sum(number_of_section_8_transition_apartments),\n            totalapts = sum(total_number_of_apartments),\n            median_section8apts = median(number_of_section_8_transition_apartments),\n            avg_section8apts = mean(number_of_section_8_transition_apartments)) \n\n# Is this all the info we need? What's missing? (Normalization)\n\n# Section 8 units as a share of all units in mixed finance developments, by borough\nsection8devs_grouped &lt;- section8devs %&gt;%\n  group_by(borough) %&gt;%\n  summarize(total_s8_apts = sum(number_of_section_8_transition_apartments),\n            total_apts = sum(total_number_of_apartments))%&gt;%\n  mutate(s8_share = total_s8_apts / total_apts)\n\n# Section 8 units as a share of all NYCHA units, by borough\nnycha_grouped &lt;- nycha %&gt;%\n  group_by(borough) %&gt;%\n  summarize(total_s8_apts = sum(number_of_section_8_transition_apartments, na.rm=TRUE),\n            total_apts = sum(total_number_of_apartments, na.rm=TRUE))%&gt;%\n  mutate(s8_share = total_s8_apts / total_apts)",
    "crumbs": [
      "Learning R",
      "8. Group By and Summarize"
    ]
  },
  {
    "objectID": "33.labeling.html",
    "href": "33.labeling.html",
    "title": "33. Labeling with Multiple Attributes",
    "section": "",
    "text": "Use an expression to create a label based on multiple columns. Via KlasKarlsson on Youtube.",
    "crumbs": [
      "QGIS",
      "33. Labeling with Multiple Attributes"
    ]
  },
  {
    "objectID": "33.labeling.html#video-tutorial",
    "href": "33.labeling.html#video-tutorial",
    "title": "33. Labeling with Multiple Attributes",
    "section": "",
    "text": "Use an expression to create a label based on multiple columns. Via KlasKarlsson on Youtube.",
    "crumbs": [
      "QGIS",
      "33. Labeling with Multiple Attributes"
    ]
  },
  {
    "objectID": "6.select_filter.html#select",
    "href": "6.select_filter.html#select",
    "title": "6. Select and Filter",
    "section": "Select",
    "text": "Select\nLet’s read in our data and do some cleaning up of the names with the pipe\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nfhv_clean &lt;- read_csv(file = \"For_Hire_Vehicles__FHV__-_Active.csv\") %&gt;% \n  clean_names() %&gt;% \n  rename(hybrid = veh)\n\nWe have a lot of information in this data frame. What if we want to look at just a few rows and columns. Two core dplyr functions, select and filter, help us do so. dplyr is a core part of the tidyverse, and it has functions that modify dataframes (think of the pipe!)\nLet’s try just keeping active, vehicle_lisence_number, name, license_type, vehicle_year, base_name, and base_type. Select’s first argument is the dataframe, and the following arguments are all the names of columns. In R documentation, an ellipses argument ... means that the function takes a list of arguments. In this case, a list of variables to select\n\nfhv_clean %&gt;% \n  select(active, vehicle_license_number, name, license_type, vehicle_year, base_name, base_type)\n\n# A tibble: 98,318 × 7\n   active vehicle_license_number name        license_type vehicle_year base_name\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;    \n 1 YES    5608977                AMERICAN,U… FOR HIRE VE…         2015 UBER USA…\n 2 YES    5645622                RAMA,ILIR   FOR HIRE VE…         2022 UBER USA…\n 3 YES    5192507                ORDONEZ,EL… FOR HIRE VE…         2016 UBER USA…\n 4 YES    5378856                RIVERA,ENMA FOR HIRE VE…         2018 BELL LX …\n 5 YES    5852121                A/VA,SERVI… FOR HIRE VE…         2019 BAYRIDGE…\n 6 YES    5415237                REYES,JUAN… FOR HIRE VE…         2012 FIRST CL…\n 7 YES    5643301                BEGUM,TAZM… FOR HIRE VE…         2015 UBER USA…\n 8 YES    5701439                GONZALEZAL… FOR HIRE VE…         2016 UBER USA…\n 9 YES    5790931                GOMEZ,JOSE… FOR HIRE VE…         2017 UBER USA…\n10 YES    5743759                HOSSAIN,SM… FOR HIRE VE…         2021 TRI-CITY…\n# ℹ 98,308 more rows\n# ℹ 1 more variable: base_type &lt;chr&gt;\n\n#this dataframe has all our observations, but only 6 variables (columns)\n\nFor more advanced selection, check out the logical operations using the tidy-select expressions. Check what - does, for instance.\n\nfhv_clean %&gt;% \n  select(-active)\n\n# A tibble: 98,318 × 22\n   vehicle_license_number name                      license_type expiration_date\n   &lt;chr&gt;                  &lt;chr&gt;                     &lt;chr&gt;        &lt;chr&gt;          \n 1 5608977                AMERICAN,UNITED,TRANSPOR… FOR HIRE VE… 04/30/2025     \n 2 5645622                RAMA,ILIR                 FOR HIRE VE… 09/11/2023     \n 3 5192507                ORDONEZ,ELIAS             FOR HIRE VE… 03/08/2025     \n 4 5378856                RIVERA,ENMA               FOR HIRE VE… 11/12/2024     \n 5 5852121                A/VA,SERVICE,CORP         FOR HIRE VE… 04/11/2024     \n 6 5415237                REYES,JUAN,E              FOR HIRE VE… 10/31/2023     \n 7 5643301                BEGUM,TAZMINUR            FOR HIRE VE… 09/30/2025     \n 8 5701439                GONZALEZALVARADO,L        FOR HIRE VE… 06/13/2024     \n 9 5790931                GOMEZ,JOSE,A              FOR HIRE VE… 05/23/2025     \n10 5743759                HOSSAIN,SM,KAMAL          FOR HIRE VE… 12/08/2024     \n# ℹ 98,308 more rows\n# ℹ 18 more variables: permit_license_number &lt;chr&gt;,\n#   dmv_license_plate_number &lt;chr&gt;, vehicle_vin_number &lt;chr&gt;,\n#   wheelchair_accessible &lt;chr&gt;, certification_date &lt;chr&gt;, hack_up_date &lt;chr&gt;,\n#   vehicle_year &lt;dbl&gt;, base_number &lt;chr&gt;, base_name &lt;chr&gt;, base_type &lt;chr&gt;,\n#   hybrid &lt;chr&gt;, base_telephone_number &lt;chr&gt;, website &lt;chr&gt;,\n#   base_address &lt;chr&gt;, reason &lt;chr&gt;, order_date &lt;lgl&gt;, …",
    "crumbs": [
      "Learning R",
      "6. Select and Filter"
    ]
  },
  {
    "objectID": "6.select_filter.html#filter",
    "href": "6.select_filter.html#filter",
    "title": "6. Select and Filter",
    "section": "Filter",
    "text": "Filter\nFilter does the same thing as select, but for rows that meet certain logical conditions. Let’s get all the uber vehicles. The first argument of filter is the dataframe. The second is a logical expression.\n\nfhv_clean %&gt;% \n  filter(base_name == \"UBER USA, LLC\")\n\n# A tibble: 76,710 × 23\n   active vehicle_license_number name               license_type expiration_date\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;          \n 1 YES    5608977                AMERICAN,UNITED,T… FOR HIRE VE… 04/30/2025     \n 2 YES    5645622                RAMA,ILIR          FOR HIRE VE… 09/11/2023     \n 3 YES    5192507                ORDONEZ,ELIAS      FOR HIRE VE… 03/08/2025     \n 4 YES    5643301                BEGUM,TAZMINUR     FOR HIRE VE… 09/30/2025     \n 5 YES    5701439                GONZALEZALVARADO,L FOR HIRE VE… 06/13/2024     \n 6 YES    5790931                GOMEZ,JOSE,A       FOR HIRE VE… 05/23/2025     \n 7 YES    5867611                HUSSAIN, TARIQ     FOR HIRE VE… 05/08/2024     \n 8 YES    5869802                LU,GUI,ZHAO        FOR HIRE VE… 05/12/2024     \n 9 YES    5715034                LI,PEI             FOR HIRE VE… 08/15/2024     \n10 YES    5725892                HAILE,TEMESGEN,K   FOR HIRE VE… 09/23/2024     \n# ℹ 76,700 more rows\n# ℹ 18 more variables: permit_license_number &lt;chr&gt;,\n#   dmv_license_plate_number &lt;chr&gt;, vehicle_vin_number &lt;chr&gt;,\n#   wheelchair_accessible &lt;chr&gt;, certification_date &lt;chr&gt;, hack_up_date &lt;chr&gt;,\n#   vehicle_year &lt;dbl&gt;, base_number &lt;chr&gt;, base_name &lt;chr&gt;, base_type &lt;chr&gt;,\n#   hybrid &lt;chr&gt;, base_telephone_number &lt;chr&gt;, website &lt;chr&gt;,\n#   base_address &lt;chr&gt;, reason &lt;chr&gt;, order_date &lt;lgl&gt;, …\n\n#this dataframe has fewer rows because we have only kept the registered Ubers.\n\nYou use R’s logical operators to return the rows that you care about. Here I’ve returned all the rows where the base_name column exactly matches the string “UBER USA, LLC.” Always use == for logical expressions. The single equals sign = is just for defining the names of arguments and other list items, and will confuse R.\nHere’s some other helpful logical operators you may find yourself using, to return certain strings, numbers, or lists.\n\nfhv_clean %&gt;% \n  filter(base_name %in% c(\"UBER USA, LLC\", \"Take Me 2 Inc\"), #name is in the list\n         vehicle_year &gt;= 2000, #year is greater than or equal to\n         hybrid != \"HYB\" #no hybrids\n         )\n\n# A tibble: 6,433 × 23\n   active vehicle_license_number name               license_type expiration_date\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;          \n 1 YES    6025256                ALSAHYBI, SUHAIB   FOR HIRE VE… 04/17/2025     \n 2 YES    5707125                CITY,QUEENS,INC    FOR HIRE VE… 07/12/2024     \n 3 YES    5278357                LI,LIN             FOR HIRE VE… 11/01/2023     \n 4 YES    6015005                GULATI,SONU        FOR HIRE VE… 01/23/2025     \n 5 YES    5839092                WILSON',SONS,INC   FOR HIRE VE… 12/28/2023     \n 6 YES    5837702                AMERICAN,UNITED,T… FOR HIRE VE… 12/18/2023     \n 7 YES    6036945                CCM NY LLC         FOR HIRE VE… 08/02/2025     \n 8 YES    6002683                WU, JINXIANG       FOR HIRE VE… 08/23/2024     \n 9 YES    5999878                ALL GREEN HAMSAF … FOR HIRE VE… 08/08/2024     \n10 YES    5661911                SINGH,SANDEEP      FOR HIRE VE… 12/16/2023     \n# ℹ 6,423 more rows\n# ℹ 18 more variables: permit_license_number &lt;chr&gt;,\n#   dmv_license_plate_number &lt;chr&gt;, vehicle_vin_number &lt;chr&gt;,\n#   wheelchair_accessible &lt;chr&gt;, certification_date &lt;chr&gt;, hack_up_date &lt;chr&gt;,\n#   vehicle_year &lt;dbl&gt;, base_number &lt;chr&gt;, base_name &lt;chr&gt;, base_type &lt;chr&gt;,\n#   hybrid &lt;chr&gt;, base_telephone_number &lt;chr&gt;, website &lt;chr&gt;,\n#   base_address &lt;chr&gt;, reason &lt;chr&gt;, order_date &lt;lgl&gt;, …\n\n\nLet’s combine it to get a subsample of columns and rows based on the criteria specified and assign it for further analysis\n\nubers_thiscentury &lt;- fhv_clean %&gt;% \n  select(active, vehicle_license_number, name, license_type, vehicle_year, base_name, base_type) %&gt;% \n  filter(base_name == \"UBER USA, LLC\",\n         vehicle_year &gt;= 2000, #year is greater than or equal to\n         )",
    "crumbs": [
      "Learning R",
      "6. Select and Filter"
    ]
  },
  {
    "objectID": "10.joins.html#joining-two-tables-together",
    "href": "10.joins.html#joining-two-tables-together",
    "title": "10. Joins",
    "section": "Joining two Tables together",
    "text": "Joining two Tables together\nJoins are powerful functions that allow you to connect two datasets together through matching values. They can be useful with spatial and non-spatial data alike.\nJoins rely on “keys” that match records across different datasets. This can be something like a name or ID number.\nAs a recap of how joins work, we’re going to show a simple example of two different kinds of joins: ‘left’ joins and ‘inner’ joins. For this example, we will be using the band_members and band_instruments dataframes, which are simple, 3-row datasets that comes included in the dplyr package.\n\nlibrary(tidyverse)\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nAs you would expect, the first two arguments in a join function are the two tables you are trying to connect together. The third argument is the “join field”, which is the matching column in both datasets we will use to pair up rows.",
    "crumbs": [
      "Learning R",
      "10. Joins"
    ]
  },
  {
    "objectID": "10.joins.html#left-joins",
    "href": "10.joins.html#left-joins",
    "title": "10. Joins",
    "section": "Left Joins",
    "text": "Left Joins\n\nA left_join keeps all of the rows in the first table you specify, appending data from the second table through matching values in the specified “join field”. Let’s see how this kind of join looks with our example data:\n\nband_members_and_instruments &lt;- \n  left_join(band_members, band_instruments, by = \"name\")\n\n# When the \"join field\" column names don't match, you can use:\n# by = c(\"column1\" = \"column2\")\n\nband_members_and_instruments\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass",
    "crumbs": [
      "Learning R",
      "10. Joins"
    ]
  },
  {
    "objectID": "10.joins.html#inner-joins",
    "href": "10.joins.html#inner-joins",
    "title": "10. Joins",
    "section": "Inner Joins",
    "text": "Inner Joins\n\nAn `inner_join` keeps only the rows that have matching values between both tables in the specified “join field.” Any other rows are discarded. Let’s see how this kind of join looks with our example data:\n\nband_members_with_instruments_only &lt;- \n  inner_join(band_members, band_instruments, by = \"name\")\n\nband_members_with_instruments_only\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass",
    "crumbs": [
      "Learning R",
      "10. Joins"
    ]
  },
  {
    "objectID": "10.joins.html#full-joins",
    "href": "10.joins.html#full-joins",
    "title": "10. Joins",
    "section": "Full Joins",
    "text": "Full Joins\n\nA full_join keeps all rows from both tables, even if a row from the join field isn’t present in one of them. It shows any missing values as NA.\n\nband_members_with_or_without_instruments &lt;- \n  full_join(band_members, band_instruments, by = \"name\")\n\nband_members_with_or_without_instruments\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar",
    "crumbs": [
      "Learning R",
      "10. Joins"
    ]
  },
  {
    "objectID": "10.joins.html#troubleshooting-joins",
    "href": "10.joins.html#troubleshooting-joins",
    "title": "10. Joins",
    "section": "Troubleshooting Joins",
    "text": "Troubleshooting Joins\n\nJoining when matching columns have different names\nOften times, the “join field” in your first table has a different name than that of your second table. For example, you may be trying to join two tables on a common zip code, but the first table calls the column ‘Zip’ and the second table calls it ‘Postal Code’. There’s a special syntax here to make it work:\n\n# Let's change the \"name\" column to be called \"MusicalArtist\"\n\nband_instruments_renamed &lt;- band_instruments %&gt;% rename(MusicalArtist = name)\n\n# In our join function, we need to specify that the \"name\" column in the first table matches up with the \"MusicalArtist\" column in the second table. We do that by setting our \"by\" parameter differently:\n\nband_members_and_instruments_2 &lt;- \n  inner_join(band_members, band_instruments_renamed, \n             by = c(\"name\" = \"MusicalArtist\"))\n\nband_members_and_instruments_2\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nYou can also join across multiple keys (variables), by giving by a list. It will return only rows that match both variables.\n\n\nDetecting duplicate values\nDuplicate values in your data can cause problems with joins. For example, what if our `band_instruments` dataset listed “John” twice:\n\n# Let's add a new row to band_instruments… say \"John\" also plays \"flute\"\n\nband_instruments_with_dup &lt;- band_instruments %&gt;% add_row(name = \"John\", plays = \"flute\")\n\nband_instruments_with_dup\n\n# A tibble: 4 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n4 John  flute \n\n\nWhen we try and join our `band_members` table with this new table, we now get 4 rows in the resulting table, even though our first table only had 3 rows… how can that be?\n\nband_members_and_instruments_dup &lt;- \n  left_join(band_members, band_instruments_with_dup, by = \"name\")\n\nband_members_and_instruments_dup\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 John  Beatles flute \n4 Paul  Beatles bass  \n\n\nIn R, if there are multiple matches between the two tables, all combinations of the matches are returned. This GIF illustrates visually how this works:\n\nIn general, to avoid confusing and unexpected results like this, it’s important to always check for duplicate values in your data, _especially_ in columns that you intend to use as a join field.\nLuckily, the `janitor` package has a function for that called `get_dupes`.\n\nlibrary(janitor)# Remember to run `install.packages('janitor')` in your console if you've\n\nband_instruments_with_dup %&gt;% get_dupes(name)\n\n# A tibble: 2 × 3\n  name  dupe_count plays \n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; \n1 John           2 guitar\n2 John           2 flute \n\n\nSometimes you may expect duplicate values in a column, and sometimes they may come as a surprise. General Rule: always know what each row represents in your data and what should be unique values.\nSometimes you’ll want to return more rows than you start with, for example if you were matching census divisions to states in order to aggregate the total land area.\n\nregions_divisions #crosswalk between regions and divisions\n\n         region           division\n1         South East South Central\n2          West            Pacific\n3          West           Mountain\n4         South West South Central\n5     Northeast        New England\n6         South     South Atlantic\n7 North Central East North Central\n8 North Central West North Central\n9     Northeast    Middle Atlantic\n\nstate_area #this doesn't have the region included! oh no!\n\n             name           division   area\n1         Alabama East South Central  51609\n2          Alaska            Pacific 589757\n3         Arizona           Mountain 113909\n4        Arkansas West South Central  53104\n5      California            Pacific 158693\n6        Colorado           Mountain 104247\n7     Connecticut        New England   5009\n8        Delaware     South Atlantic   2057\n9         Florida     South Atlantic  58560\n10        Georgia     South Atlantic  58876\n11         Hawaii            Pacific   6450\n12          Idaho           Mountain  83557\n13       Illinois East North Central  56400\n14        Indiana East North Central  36291\n15           Iowa West North Central  56290\n16         Kansas West North Central  82264\n17       Kentucky East South Central  40395\n18      Louisiana West South Central  48523\n19          Maine        New England  33215\n20       Maryland     South Atlantic  10577\n21  Massachusetts        New England   8257\n22       Michigan East North Central  58216\n23      Minnesota West North Central  84068\n24    Mississippi East South Central  47716\n25       Missouri West North Central  69686\n26        Montana           Mountain 147138\n27       Nebraska West North Central  77227\n28         Nevada           Mountain 110540\n29  New Hampshire        New England   9304\n30     New Jersey    Middle Atlantic   7836\n31     New Mexico           Mountain 121666\n32       New York    Middle Atlantic  49576\n33 North Carolina     South Atlantic  52586\n34   North Dakota West North Central  70665\n35           Ohio East North Central  41222\n36       Oklahoma West South Central  69919\n37         Oregon            Pacific  96981\n38   Pennsylvania    Middle Atlantic  45333\n39   Rhode Island        New England   1214\n40 South Carolina     South Atlantic  31055\n41   South Dakota West North Central  77047\n42      Tennessee East South Central  42244\n43          Texas West South Central 267339\n44           Utah           Mountain  84916\n45        Vermont        New England   9609\n46       Virginia     South Atlantic  40815\n47     Washington            Pacific  68192\n48  West Virginia     South Atlantic  24181\n49      Wisconsin East North Central  56154\n50        Wyoming           Mountain  97914\n\nregions_states &lt;- left_join(regions_divisions, state_area, by = \"division\")\n\nregions_states #the join matches state to region and lets us summarize\n\n          region           division           name   area\n1          South East South Central        Alabama  51609\n2          South East South Central       Kentucky  40395\n3          South East South Central    Mississippi  47716\n4          South East South Central      Tennessee  42244\n5           West            Pacific         Alaska 589757\n6           West            Pacific     California 158693\n7           West            Pacific         Hawaii   6450\n8           West            Pacific         Oregon  96981\n9           West            Pacific     Washington  68192\n10          West           Mountain        Arizona 113909\n11          West           Mountain       Colorado 104247\n12          West           Mountain          Idaho  83557\n13          West           Mountain        Montana 147138\n14          West           Mountain         Nevada 110540\n15          West           Mountain     New Mexico 121666\n16          West           Mountain           Utah  84916\n17          West           Mountain        Wyoming  97914\n18         South West South Central       Arkansas  53104\n19         South West South Central      Louisiana  48523\n20         South West South Central       Oklahoma  69919\n21         South West South Central          Texas 267339\n22     Northeast        New England    Connecticut   5009\n23     Northeast        New England          Maine  33215\n24     Northeast        New England  Massachusetts   8257\n25     Northeast        New England  New Hampshire   9304\n26     Northeast        New England   Rhode Island   1214\n27     Northeast        New England        Vermont   9609\n28         South     South Atlantic       Delaware   2057\n29         South     South Atlantic        Florida  58560\n30         South     South Atlantic        Georgia  58876\n31         South     South Atlantic       Maryland  10577\n32         South     South Atlantic North Carolina  52586\n33         South     South Atlantic South Carolina  31055\n34         South     South Atlantic       Virginia  40815\n35         South     South Atlantic  West Virginia  24181\n36 North Central East North Central       Illinois  56400\n37 North Central East North Central        Indiana  36291\n38 North Central East North Central       Michigan  58216\n39 North Central East North Central           Ohio  41222\n40 North Central East North Central      Wisconsin  56154\n41 North Central West North Central           Iowa  56290\n42 North Central West North Central         Kansas  82264\n43 North Central West North Central      Minnesota  84068\n44 North Central West North Central       Missouri  69686\n45 North Central West North Central       Nebraska  77227\n46 North Central West North Central   North Dakota  70665\n47 North Central West North Central   South Dakota  77047\n48     Northeast    Middle Atlantic     New Jersey   7836\n49     Northeast    Middle Atlantic       New York  49576\n50     Northeast    Middle Atlantic   Pennsylvania  45333\n\nregions_states %&gt;% \n  group_by(region) %&gt;% \n  summarize(region_area = sum(area))\n\n# A tibble: 4 × 2\n  region        region_area\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Northeast          169353\n2 South              899556\n3 North Central      765530\n4 West              1783960",
    "crumbs": [
      "Learning R",
      "10. Joins"
    ]
  },
  {
    "objectID": "35.inset_maps.html",
    "href": "35.inset_maps.html",
    "title": "35. Inset Maps and Labels",
    "section": "",
    "text": "Building off of the last tutorial of a subway walkability map, add labels of station names and an inset reference map to show the extent of the main map within all of New York City.",
    "crumbs": [
      "QGIS",
      "35. Inset Maps and Labels"
    ]
  },
  {
    "objectID": "35.inset_maps.html#video-tutorial",
    "href": "35.inset_maps.html#video-tutorial",
    "title": "35. Inset Maps and Labels",
    "section": "",
    "text": "Building off of the last tutorial of a subway walkability map, add labels of station names and an inset reference map to show the extent of the main map within all of New York City.",
    "crumbs": [
      "QGIS",
      "35. Inset Maps and Labels"
    ]
  },
  {
    "objectID": "24.calculate_field.html",
    "href": "24.calculate_field.html",
    "title": "24. Calculating a new field",
    "section": "",
    "text": "Calculate a new field (a.k.a. variable or column) of population per healthcare facility for each NYC community district using the Field Calculator tool",
    "crumbs": [
      "QGIS",
      "24. Calculating a new field"
    ]
  },
  {
    "objectID": "24.calculate_field.html#video-tutorial",
    "href": "24.calculate_field.html#video-tutorial",
    "title": "24. Calculating a new field",
    "section": "",
    "text": "Calculate a new field (a.k.a. variable or column) of population per healthcare facility for each NYC community district using the Field Calculator tool",
    "crumbs": [
      "QGIS",
      "24. Calculating a new field"
    ]
  },
  {
    "objectID": "24.calculate_field.html#data-downloads",
    "href": "24.calculate_field.html#data-downloads",
    "title": "24. Calculating a new field",
    "section": "Data downloads",
    "text": "Data downloads\nShapefile of community districts with healthcare facilities and population",
    "crumbs": [
      "QGIS",
      "24. Calculating a new field"
    ]
  },
  {
    "objectID": "32.points_in_polygon.html",
    "href": "32.points_in_polygon.html",
    "title": "32. Count Points in Polygon",
    "section": "",
    "text": "Count the number of points in one layer that fall within the polygon boundaries of another layer. In this case, count the number of COVID-19 free meal locations within each City Council district.",
    "crumbs": [
      "QGIS",
      "32. Count Points in Polygon"
    ]
  },
  {
    "objectID": "32.points_in_polygon.html#video-tutorial",
    "href": "32.points_in_polygon.html#video-tutorial",
    "title": "32. Count Points in Polygon",
    "section": "",
    "text": "Count the number of points in one layer that fall within the polygon boundaries of another layer. In this case, count the number of COVID-19 free meal locations within each City Council district.",
    "crumbs": [
      "QGIS",
      "32. Count Points in Polygon"
    ]
  },
  {
    "objectID": "32.points_in_polygon.html#data-downloads",
    "href": "32.points_in_polygon.html#data-downloads",
    "title": "32. Count Points in Polygon",
    "section": "Data downloads",
    "text": "Data downloads\nCOVID-19 Free Meals Locations\nNYC Council Districts",
    "crumbs": [
      "QGIS",
      "32. Count Points in Polygon"
    ]
  },
  {
    "objectID": "30.geoprocessing.html",
    "href": "30.geoprocessing.html",
    "title": "30. Geoprocessing: buffers, select by location, and clipping",
    "section": "",
    "text": "Create a buffer of 1/2 mile around a point layer of subway stations, select all NYC tax lots that intersect with that buffer, and calculate the total residential units inside the buffer as a share of all residential units in NYC. Also, Clip the MapPLUTO layer of tax lots by the buffer for a neater visualization.",
    "crumbs": [
      "QGIS",
      "30. Geoprocessing: buffers, select by location, and clipping"
    ]
  },
  {
    "objectID": "30.geoprocessing.html#video-tutorial",
    "href": "30.geoprocessing.html#video-tutorial",
    "title": "30. Geoprocessing: buffers, select by location, and clipping",
    "section": "",
    "text": "Create a buffer of 1/2 mile around a point layer of subway stations, select all NYC tax lots that intersect with that buffer, and calculate the total residential units inside the buffer as a share of all residential units in NYC. Also, Clip the MapPLUTO layer of tax lots by the buffer for a neater visualization.",
    "crumbs": [
      "QGIS",
      "30. Geoprocessing: buffers, select by location, and clipping"
    ]
  },
  {
    "objectID": "30.geoprocessing.html#data-downloads",
    "href": "30.geoprocessing.html#data-downloads",
    "title": "30. Geoprocessing: buffers, select by location, and clipping",
    "section": "Data downloads",
    "text": "Data downloads\nMapPLUTO\nNYC subway stations (Saved copy)",
    "crumbs": [
      "QGIS",
      "30. Geoprocessing: buffers, select by location, and clipping"
    ]
  },
  {
    "objectID": "31.join_by_location.html",
    "href": "31.join_by_location.html",
    "title": "31. Join by location",
    "section": "",
    "text": "Join attributes of one layer to another based on their spatial relationship. In this case, join City Council district numbers to the layer of COVID-19 free meal locations.",
    "crumbs": [
      "QGIS",
      "31. Join by location"
    ]
  },
  {
    "objectID": "31.join_by_location.html#video-tutorial",
    "href": "31.join_by_location.html#video-tutorial",
    "title": "31. Join by location",
    "section": "",
    "text": "Join attributes of one layer to another based on their spatial relationship. In this case, join City Council district numbers to the layer of COVID-19 free meal locations.",
    "crumbs": [
      "QGIS",
      "31. Join by location"
    ]
  },
  {
    "objectID": "31.join_by_location.html#data-downloads",
    "href": "31.join_by_location.html#data-downloads",
    "title": "31. Join by location",
    "section": "Data downloads",
    "text": "Data downloads\nCOVID-19 Free Meals Locations\nNYC Council Districts",
    "crumbs": [
      "QGIS",
      "31. Join by location"
    ]
  }
]